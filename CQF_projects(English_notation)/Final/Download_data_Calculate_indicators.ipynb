{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import certifi\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "import yfinance as yf\n",
    "import os\n",
    "import pandas_datareader.data as web\n",
    "from pandas_datareader import wb\n",
    "import requests\n",
    "\n",
    "# feature extraction\n",
    "from talib.abstract import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "import mplfinance as mpf\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Download all the data I need and put them in one csv.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 5000)\n",
    "# pd.set_option('display.max_columns', 5000)\n",
    "# pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for certain stcok "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic daily data\n",
    "def dl_basic(symbol):\n",
    "    data = yf.download(symbol, auto_adjust=True, start=\"2010-01-04\", end=\"2022-06-30\")\n",
    "    data.to_csv(r'./data/'+symbol+'/'+symbol+'.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### data from Financial Modeling Prep api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jsonparsed_data(url):\n",
    "    response = urlopen(url, cafile=certifi.where())\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three statements, financial ratio and enterprise value \n",
    "def dl_financial(symbol):\n",
    "    name_statement_pair = {'BS':'balance-sheet-statement',\n",
    "                           'IS':'income-statement',\n",
    "                           'CF':'cash-flow-statement',\n",
    "                           'FR':'ratios',\n",
    "                           'EV':'enterprise-values'\n",
    "                          }\n",
    "    for name, statement in name_statement_pair.items():\n",
    "        url = ('https://financialmodelingprep.com/api/v3/'+statement+'/'+symbol+'?period=quarter&limit=52&apikey=')\n",
    "        data = get_jsonparsed_data(url)\n",
    "        with open('./data/'+symbol+'/Financial/'+name+'.json', 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of employees\n",
    "# Not all companies disclose this number\n",
    "# def dl_employees(symbol):\n",
    "#     url = ('https://financialmodelingprep.com/api/v4/historical/employee_count?symbol='+symbol+'&apikey=')\n",
    "#     data = get_jsonparsed_data(url)\n",
    "#     Employees = pd.DataFrame({'date':[],\n",
    "#                              'num_employees':[]})\n",
    "#     for i in data:\n",
    "#         Employees = Employees.append(pd.DataFrame({'date':[i['filingDate']],\n",
    "#              'Employees':[i['employeeCount']]}))\n",
    "\n",
    "#     Employees.to_csv(r'./data/'+symbol+'/event/Employees.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESG json的版本\n",
    "def dl_ESG(symbol):\n",
    "    url = ('https://financialmodelingprep.com/api/v4/esg-environmental-social-governance-data?symbol='+symbol+'&apikey=')\n",
    "    data = get_jsonparsed_data(url)\n",
    "\n",
    "    with open('./data/'+symbol+'/event/ESG.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''peer companies' ohlc and volume, I use peer companies chosen by FMP'''\n",
    "def dl_peers_basic(symbol):\n",
    "    url = ('https://financialmodelingprep.com/api/v4/stock_peers?symbol='+symbol+'&apikey=')\n",
    "    peers = get_jsonparsed_data(url)\n",
    "    peer_list = peers[0]['peersList']\n",
    "    for i in peer_list:\n",
    "        data = yf.download(i, auto_adjust=True, start=\"2010-01-04\", end=\"2022-06-30\")\n",
    "        data.to_csv(r'./data/'+symbol+'/peers/'+i+'.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insider trading\n",
    "def dl_insider_trading(symbol, pages):\n",
    "    insider_trading = []\n",
    "    for page in range(pages):\n",
    "        url = ('https://financialmodelingprep.com/api/v4/insider-trading?symbol='+symbol+'&page='+str(page)+'&apikey=')\n",
    "        data = get_jsonparsed_data(url)\n",
    "        for i in range(len(data)):\n",
    "            if data[i]['transactionType'] == 'P-Purchase' or data[i]['transactionType'] == 'S-Sale':\n",
    "                insider_trading.append(data[i])\n",
    "    with open('./data/'+symbol+'/event/Insider_trading.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(insider_trading, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# senate trading\n",
    "def dl_senate_trading(symbol):\n",
    "    url = ('https://financialmodelingprep.com/api/v4/senate-trading?symbol='+symbol+'&apikey=')\n",
    "    data = get_jsonparsed_data(url)\n",
    "    with open('./data/'+symbol+'/event/Senate_trading.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### download all stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_stock_data(symbol, pages):\n",
    "    if not os.path.isdir(r'./data/'+symbol):\n",
    "        os.mkdir(r'./data/'+symbol)\n",
    "    if not os.path.isdir(r'./data/'+symbol+'/event'):\n",
    "        os.mkdir(r'./data/'+symbol+'/event')\n",
    "    if not os.path.isdir(r'./data/'+symbol+'/Financial'):\n",
    "        os.mkdir(r'./data/'+symbol+'/Financial')\n",
    "    if not os.path.isdir(r'./data/'+symbol+'/peers'):\n",
    "        os.mkdir(r'./data/'+symbol+'/peers')\n",
    "    dl_basic(symbol)\n",
    "    dl_financial(symbol)\n",
    "#     dl_employees(symbol)\n",
    "    dl_ESG(symbol)\n",
    "    dl_peers_basic(symbol)\n",
    "    dl_insider_trading(symbol, pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF factor\n",
    "# def df_FF():\n",
    "#     FFDataTest = pdr.data.DataReader(\"F-F_Research_Data_5_Factors_2x3_daily\", \"famafrench\", start='2010')[0]\n",
    "#     FFDataTest.index = pd.to_datetime(FFDataTest.index, format=\"%Y%m%d\", utc=True)\n",
    "#     FFDataTest.to_csv(r'FFDataTest.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Alpha Vantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPI\n",
    "# def dl_CPI():\n",
    "#     url = 'https://www.alphavantage.co/query?function=CPI&interval=monthly&apikey=Y2IBWUQEKYT0GVR'\n",
    "#     r = requests.get(url)\n",
    "#     data = r.json()\n",
    "\n",
    "#     date_list = []\n",
    "#     value_list = []\n",
    "#     for i in data['data']:\n",
    "#         date_list.append(i['date'])\n",
    "#         value_list.append(i['value'])\n",
    "#     CPI = pd.DataFrame({'date':date_list,'CPI':value_list})\n",
    "#     CPI.to_csv(r'./data/CPI.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock index\n",
    "# def dl_index():\n",
    "#     data = yf.download('^VIX', start=\"2010-01-01\", end=\"2022-07-01\")\n",
    "#     data.to_csv(r'./data/VIX.csv',sep=',')\n",
    "#     data = yf.download('^DJI', start=\"2010-01-01\", end=\"2022-07-01\")\n",
    "#     data.to_csv(r'./data/DJI.csv',sep=',')\n",
    "#     data = yf.download('^IXIC', start=\"2010-01-01\", end=\"2022-07-01\")\n",
    "#     data.to_csv(r'./data/NASDAQ.csv',sep=',')\n",
    "#     data = yf.download('^GSPC', start=\"2010-01-01\", end=\"2022-07-01\")\n",
    "#     data.to_csv(r'./data/S&P.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# futures\n",
    "# def dl_futures():\n",
    "#     data = yf.download('CL=F', start=\"2010-01-01\", end=\"2022-07-01\")\n",
    "#     data.to_csv(r'./data/OIL_Future.csv',sep=',')\n",
    "#     data = yf.download('HG=F', start=\"2010-01-01\", end=\"2022-07-01\")\n",
    "#     data.to_csv(r'./data/COPPER_Future.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commodities\n",
    "# def dl_commodities():\n",
    "#     url = ('https://financialmodelingprep.com/api/v3/historical-price-full/CLUSD?apikey=')\n",
    "#     data = get_jsonparsed_data(url)\n",
    "#     with open('./data/Crude_Oil_Commodity.json', 'w', encoding='utf-8') as json_file:\n",
    "#         json.dump(data, json_file, ensure_ascii=False)\n",
    "        \n",
    "#     url = ('https://financialmodelingprep.com/api/v3/historical-price-full/HGUSD?apikey=')\n",
    "#     data = get_jsonparsed_data(url)\n",
    "#     with open('./data/Copper_Commodity.json', 'w', encoding='utf-8') as json_file:\n",
    "#         json.dump(data, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_kandle(symbol):\n",
    "    # 创建文件夹\n",
    "    save_path = r'./data/'+symbol+'/images_2'\n",
    "    if os.path.exists(save_path):\n",
    "        shutil.rmtree(save_path)\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "#     设置mplfinance的蜡烛颜色，up为阳线颜色，down为阴线颜色\n",
    "#     my_color = mpf.make_marketcolors(up='r',\n",
    "#                                      down='g',\n",
    "#                                      edge='inherit',\n",
    "#                                      wick='inherit',\n",
    "#                                      volume='inherit')\n",
    "#     设置图表的背景色\n",
    "    my_style = mpf.make_mpf_style(figcolor='(0, 0, 0)')\n",
    "\n",
    "    data = get_basic_data(symbol)\n",
    "\n",
    "    # 画图并保存\n",
    "    window = 30\n",
    "    for i in range(len(data)-window+1):\n",
    "        df = data[i: i+window]\n",
    "        start = df[0:1].index[0].strftime('%Y-%m-%d')\n",
    "        end = df[-1:].index[0].strftime('%Y-%m-%d')\n",
    "        savefig = r'./data/'+symbol+'/images_2/' + start + '_' + end + '.jpg'\n",
    "        mpf.plot(df, style=my_style, volume=True, savefig=savefig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kandle('CSX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_data(symbol):\n",
    "    path = r'./data/'+symbol+'/'+symbol+'.csv'\n",
    "    data = pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "    data.index = pd.to_datetime(data.index, format=\"%Y%m%d\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "using filling date as the date when the market can have access to the financial report\n",
    "'''\n",
    "def get_Fundamental(data, symbol, indicator_list):\n",
    "    with open('./data/'+symbol+'/Financial/FR.json', 'r', encoding='utf-8') as json_file:\n",
    "        FR = json.load(json_file)\n",
    "    with open('./data/'+symbol+'/Financial/BS.json', 'r', encoding='utf-8') as json_file:\n",
    "        BS = json.load(json_file)\n",
    "    with open('./data/'+symbol+'/Financial/IS.json', 'r', encoding='utf-8') as json_file:\n",
    "        IS = json.load(json_file)\n",
    "    with open('./data/'+symbol+'/Financial/EV.json', 'r', encoding='utf-8') as json_file:\n",
    "        EV = json.load(json_file)\n",
    "\n",
    "    for i in FR:\n",
    "        for j in [BS, IS, EV]:\n",
    "            for k in j:\n",
    "                if i['date'] == k['date']:\n",
    "                    for m,n in k.items():\n",
    "                        i[m] = n\n",
    "                        \n",
    "                        \n",
    "    # 颠倒顺序是为了填充dataframe方便\n",
    "    FR.reverse()\n",
    "\n",
    "    '''\n",
    "    因为FR中的字典是按照日期从小到大排列的，从FR中选出一组数据i，data中日期大于i，小于i+1的数据就会按照第i组数据的财报来赋值\n",
    "    '''\n",
    "    for i in range(len(FR)):\n",
    "        for j in data.index:\n",
    "            if i+1 < len(FR):\n",
    "                if j >= datetime.strptime(FR[i+1]['fillingDate'], \"%Y-%m-%d\"):\n",
    "                    continue\n",
    "            if j >= datetime.strptime(FR[i]['fillingDate'], \"%Y-%m-%d\"):\n",
    "                for k in indicator_list:\n",
    "                    data.loc[j, k] = FR[i][k]\n",
    "                # PE\n",
    "                data.loc[j, 'PE'] = data.loc[j, 'Close']/(FR[i]['netIncome']/FR[i]['numberOfShares'])\n",
    "                # PB\n",
    "                data.loc[j, 'PB'] = data.loc[j, 'Close']/(FR[i]['revenue']/FR[i]['numberOfShares'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ESG(data, symbol):\n",
    "    with open('./data/'+symbol+'/event/ESG.json', 'r', encoding='utf-8') as json_file:\n",
    "        ESG = json.load(json_file)\n",
    "    ESG.reverse()\n",
    "    data['ESG'] = 0\n",
    "    for i in range(len(ESG)):\n",
    "        for j in data.index:\n",
    "            if i+1 < len(ESG):\n",
    "                if j >= datetime.strptime(ESG[i+1]['acceptedDate'][:10], \"%Y-%m-%d\"):\n",
    "                    continue\n",
    "            if j >= datetime.strptime(ESG[i]['acceptedDate'][:10], \"%Y-%m-%d\"):\n",
    "                data.loc[j, 'ESG'] = ESG[i]['ESGScore']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peers_data(data, symbol):\n",
    "    for i in os.listdir('./data/'+symbol+'/peers'):\n",
    "        df = pd.read_csv(r'./data/'+symbol+'/peers/'+i, index_col=0, parse_dates=True)\n",
    "        df.columns = ['Open','High','Low','P_'+i[:-4], 'Volume']\n",
    "        df = df['P_'+i[:-4]]\n",
    "        data = pd.merge(data, df, left_index=True, right_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is super naive, my plan is that:\n",
    "    when there is a insider purchase someday, the following five day will have a feature of value from 1 to 0.2, and negative value for sales.\n",
    "    My concern is that maybe sales are not always bad news and vice versa, so I will test another plan in which I only give positive feature\n",
    "    value no matter what transaction type it is.\n",
    "    '''\n",
    "def get_events(data, symbol):\n",
    "    with open('./data/'+symbol+'/event/Insider_trading.json', 'r', encoding='utf-8') as json_file:\n",
    "        Insider = json.load(json_file)\n",
    "    Insider.reverse()\n",
    "    data['Insider'] = 0\n",
    "    for i in range(len(Insider)):\n",
    "        decay = 0\n",
    "        for j in data.index:\n",
    "            if i+1 < len(Insider):\n",
    "                if j >= datetime.strptime(Insider[i+1]['filingDate'][:10], \"%Y-%m-%d\"):\n",
    "                    continue\n",
    "            if j >= datetime.strptime(Insider[i]['filingDate'][:10], \"%Y-%m-%d\"):\n",
    "                if Insider[i]['transactionType'] == 'P-Purchase':\n",
    "                    data.loc[j, 'Insider'] = 5 - decay\n",
    "                else:\n",
    "                    data.loc[j, 'Insider'] = -5 + decay\n",
    "                decay += 1\n",
    "                if decay == 5:\n",
    "                    break\n",
    "\n",
    "    with open('./data/'+symbol+'/event/Senate_trading.json', 'r', encoding='utf-8') as json_file:\n",
    "        Senate = json.load(json_file)\n",
    "    Senate.reverse()\n",
    "    data['Senate'] = 0\n",
    "    for i in range(len(Senate)):\n",
    "        decay = 0\n",
    "        for j in data.index:\n",
    "            if i+1 < len(Senate):\n",
    "                if j >= datetime.strptime(Senate[i+1]['dateRecieved'], \"%Y-%m-%d\"):\n",
    "                    continue\n",
    "            if j >= datetime.strptime(Senate[i]['dateRecieved'], \"%Y-%m-%d\"):\n",
    "                if Senate[i]['type'] == 'Purchase':\n",
    "                    data.loc[j, 'Senate'] = 5 - decay\n",
    "                else:\n",
    "                    data.loc[j, 'Senate'] = -5 + decay\n",
    "                decay += 1\n",
    "                if decay == 5:\n",
    "                    break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_general_data(data):\n",
    "    data.index = pd.to_datetime(data.index, format=\"%Y%m%d\", utc=True)\n",
    "    # FF factors\n",
    "    FFDataTest = pd.read_csv(r'data/FFDataTest.csv', index_col=0, parse_dates=True)\n",
    "    data = pd.merge(data, FFDataTest, left_index=True, right_index=True)\n",
    "    data = data.drop(['RF'], axis=1)\n",
    "\n",
    "    FFR = pd.read_csv(r'data/Federal_funds_rate.csv', index_col=1, parse_dates=True).drop(['Unnamed: 0'], axis=1).sort_index()\n",
    "    FFR.index = pd.to_datetime(FFR.index, format=\"%Y%m%d\", utc=True)\n",
    "    data = pd.merge(data, FFR, left_index=True, right_index=True)\n",
    "\n",
    "    CPI = pd.read_csv(r'data/CPI.csv', index_col=1, parse_dates=True).drop(['Unnamed: 0'], axis=1).sort_index()\n",
    "    CPI = CPI.loc['2010-01-01':]\n",
    "    CPI.index = pd.to_datetime(CPI.index, format=\"%Y%m%d\", utc=True)\n",
    "    for i in CPI.index:\n",
    "        for j in data.index:\n",
    "            if str(i)[:7] == str(j)[:7]:\n",
    "                data.loc[j, 'CPI'] = CPI.loc[i][0]\n",
    "\n",
    "    data_list = ['DJI', 'NASDAQ', 'S&P', 'COPPER_Future', 'OIL_Future', 'VIX']\n",
    "    for d in data_list:\n",
    "        df = pd.read_csv(r'data/'+d+'.csv', index_col=0, parse_dates=True)\n",
    "        df.drop(['Close'], axis=1, inplace=True)\n",
    "        df.columns = [d+'_'+i for i in df.columns]\n",
    "        df.index = pd.to_datetime(df.index, format=\"%Y%m%d\", utc=True)\n",
    "        data = pd.merge(data, df, left_index=True, right_index=True)\n",
    "\n",
    "    data.drop(['VIX_Volume'], axis=1, inplace=True)\n",
    "    \n",
    "#     with open('./data/Crude_Oil_Commodity.json', 'r', encoding='utf-8') as json_file:\n",
    "#         Crude_Oil_Commodity = json.load(json_file)\n",
    "#     df = pd.DataFrame({'date':[],\n",
    "#                      'Crude_Oil_Commodity':[]})\n",
    "#     for i in Crude_Oil_Commodity['historical']:\n",
    "#         df = df.append(pd.DataFrame({'date':[i['date']],\n",
    "#                                      'Crude_Oil_Commodity':[i['adjClose']]}))\n",
    "#     df = df.set_index('date')\n",
    "#     df = df.sort_index()\n",
    "#     df.index = pd.to_datetime(df.index, format=\"%Y-%m-%d\", utc=True)\n",
    "#     data = pd.merge(data, df, left_index=True, right_index=True)\n",
    "    \n",
    "#     with open('./data/Copper_Commodity.json', 'r', encoding='utf-8') as json_file:\n",
    "#         Copper_Commodity = json.load(json_file)\n",
    "#     df = pd.DataFrame({'date':[],\n",
    "#                      'Copper_Commodity':[]})\n",
    "#     for i in Copper_Commodity['historical']:\n",
    "#         df = df.append(pd.DataFrame({'date':[i['date']],\n",
    "#                                      'Copper_Commodity':[i['adjClose']]}))\n",
    "#     df = df.set_index('date')\n",
    "#     df = df.sort_index()\n",
    "#     df.index = pd.to_datetime(df.index, format=\"%Y-%m-%d\", utc=True)\n",
    "#     data = pd.merge(data, df, left_index=True, right_index=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Tecnical Indicators\n",
    "    window_list = [5, 10, 21, 50, 200]\n",
    "    for i in window_list:\n",
    "        df[f'MA_{i}'] = MA(df['Close'], i)\n",
    "        df[f'SMA_{i}'] = SMA(df['Close'], i)\n",
    "        df[f'EMA_{i}'] = EMA(df['Close'], i)\n",
    "    df['RSI_5'] = RSI(df['Close'], 5)\n",
    "    df['RSI_14'] = RSI(df['Close'], 14)\n",
    "    df['ATR_14'] = ATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
    "    df['CCI_14'] = CCI(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
    "    df['CCI_24'] = CCI(df['High'], df['Low'], df['Close'], timeperiod=24)\n",
    "    df['BBANDS_5_UB'],df['BBANDS_5_MB'],df['BBANDS_5_LB'] = BBANDS(df['Close'], timeperiod=5)\n",
    "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = MACD(df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    df['Slow_K'], df['Slow_D'] = STOCH(df['High'], df['Low'], df['Close'], fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "    df['ADOSC'] = ADOSC(df['High'], df['Low'], df['Close'], df['Volume'], fastperiod=3, slowperiod=10)\n",
    "\n",
    "    # 计算n天的YZ方差\n",
    "    n = 10\n",
    "    df['C_0'] = df['Close'].shift()     # 昨天的收盘价\n",
    "    df['o'] = np.log(df['Open']/df['C_0'])\n",
    "    df['u'] = np.log(df['High']/df['Open'])\n",
    "    df['d'] = np.log(df['Low']/df['Open'])\n",
    "    df['c'] = np.log(df['Close']/df['Open'])\n",
    "    # 计算V_RS\n",
    "    df['V_RS'] = (df['u']*(df['u']-df['c'])+df['d']*(df['d']-df['c'])).rolling(n).mean()\n",
    "    # 计算V_o\n",
    "    df['V_o'] = (df['o'] - df['o'].rolling(n).mean()).rolling(n).var()\n",
    "    # 计算V_c\n",
    "    df['V_c'] = (df['c'] - df['c'].rolling(n).mean()).rolling(n).var()\n",
    "    # 计算V_YZ\n",
    "    k = 0.34/(1.34+(n+1)/(n-1))\n",
    "    df['V_YZ'] = df['V_o'] + k*df['V_c'] + (1-k)*df['V_RS']\n",
    "    df = df.drop(['V_RS', 'V_o', 'V_c', 'o', 'u', 'd', 'c', 'C_0'], axis=1)\n",
    "\n",
    "    # Fluctuation Percentage\n",
    "    fluc_per_list = ['SMA_10', 'RSI_5', 'Slow_K', 'Slow_D']\n",
    "    for i in fluc_per_list:    \n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        df[i+'_fluc_per'] = scaler.fit_transform(df[i].pct_change().values.reshape(-1,1))\n",
    "\n",
    "    # Polarize\n",
    "    pol_list = ['MACD', 'MACD_Signal', 'MACD_Hist', 'CCI_24', 'ADOSC']\n",
    "    for i in pol_list:\n",
    "        df[i+'_pol'] = np.where(df[i]>0, 1, -1)\n",
    "\n",
    "    # Min-Max\n",
    "    MM_list = ['Volume', 'SMA_10', 'RSI_5', 'Slow_K', 'Slow_D', 'ADOSC']\n",
    "    for i in MM_list:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        df[i+'_MM'] = scaler.fit_transform(df[i].values.reshape(-1,1))\n",
    "\n",
    "    # Other Features\n",
    "    window=20\n",
    "    df.dropna(inplace=True)\n",
    "    df['r'] = np.log(df['Close'] / df['Close'].shift())\n",
    "    df['Min_20'] = df['Close'].rolling(window).min()\n",
    "    df['Max_20'] = df['Close'].rolling(window).max()\n",
    "    df['Std_20'] = df['r'].rolling(window).std()\n",
    "    df = df.drop('r', axis=1)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avengers_Assemble(symbol):\n",
    "    data = get_basic_data(symbol)\n",
    "    indicator_list = ['cashRatio', 'quickRatio', 'currentRatio', 'debtEquityRatio', 'receivablesTurnover', 'inventoryTurnover'\n",
    "                  ,'grossProfitMargin', 'netProfitMargin', 'returnOnEquity', 'dividendPayoutRatio']\n",
    "    data = get_Fundamental(data, symbol, indicator_list)\n",
    "    data = get_ESG(data,symbol)\n",
    "    data = get_peers_data(data, symbol)\n",
    "    data = get_events(data, symbol)\n",
    "    data = get_general_data(data)\n",
    "    data = add_features(data)\n",
    "\n",
    "    data.to_csv(r'./data/'+symbol+'/'+symbol+'_ready_to_use.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Avengers_Assemble('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'./data/CSX/CSX_ready_to_use.csv', index_col=0, parse_dates=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
